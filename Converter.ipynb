{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tool.config import Cfg\n",
    "from tool.translate import build_model, process_input, translate\n",
    "import torch\n",
    "import onnxruntime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Cfg.load_config_from_file('./config/vgg-seq2seq.yml')\n",
    "config['cnn']['pretrained']=False\n",
    "config['device'] = 'cpu'\n",
    "model, vocab = build_model(config)\n",
    "weight_path = './weight/transformerocr.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight\n",
    "model.load_state_dict(torch.load(weight_path, map_location=torch.device(config['device'])))\n",
    "model = model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_part(img, save_path, model, max_seq_length=128, sos_token=1, eos_token=2): \n",
    "    with torch.no_grad(): \n",
    "        src = model.cnn(img)\n",
    "        torch.onnx.export(model.cnn, img, save_path, export_params=True, opset_version=12, do_constant_folding=True, verbose=True, input_names=['img'], output_names=['output'], dynamic_axes={'img': {3: 'lenght'}, 'output': {0: 'channel'}})\n",
    "    \n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%img : Float(1, 3, 32, *, strides=[45600, 15200, 475, 1], requires_grad=0, device=cpu),\n",
      "      %model.last_conv_1x1.weight : Float(256, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %model.last_conv_1x1.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %190 : Float(64, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %191 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %193 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %196 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %197 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %199 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %200 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %202 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %203 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %205 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %206 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %208 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %209 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %211 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %212 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %214 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %215 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %217 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %218 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %220 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %221 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %223 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %224 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %226 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %227 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %229 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %230 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %232 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %233 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %235 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %236 : Float(512, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %189 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%img, %190, %191)\n",
      "  %117 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Relu(%189) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %192 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%117, %193, %194)\n",
      "  %120 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Relu(%192) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %121 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ]]()\n",
      "  %122 : Float(1, 64, 32, *, device=cpu) = onnx::Pad[mode=\"constant\"](%120, %121)\n",
      "  %123 : Float(1, 64, 16, *, strides=[242688, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%122) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/modules/pooling.py:616:0\n",
      "  %195 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%123, %196, %197)\n",
      "  %126 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Relu(%195) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %198 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%126, %199, %200)\n",
      "  %129 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Relu(%198) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %130 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ]]()\n",
      "  %131 : Float(1, 128, 16, *, device=cpu) = onnx::Pad[mode=\"constant\"](%129, %130)\n",
      "  %132 : Float(1, 128, 8, *, strides=[120832, 944, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%131) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/modules/pooling.py:616:0\n",
      "  %201 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%132, %202, %203)\n",
      "  %135 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%201) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %204 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%135, %205, %206)\n",
      "  %138 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%204) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %207 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%138, %208, %209)\n",
      "  %141 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%207) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %210 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%141, %211, %212)\n",
      "  %144 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%210) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %145 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ]]()\n",
      "  %146 : Float(1, 256, 8, *, device=cpu) = onnx::Pad[mode=\"constant\"](%144, %145)\n",
      "  %147 : Float(1, 256, 4, *, strides=[120832, 472, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, kernel_shape=[2, 1], pads=[0, 0, 0, 0], strides=[2, 1]](%146) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/modules/pooling.py:616:0\n",
      "  %213 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%147, %214, %215)\n",
      "  %150 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%213) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %216 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%150, %217, %218)\n",
      "  %153 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%216) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %219 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%153, %220, %221)\n",
      "  %156 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%219) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %222 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%156, %223, %224)\n",
      "  %159 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%222) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %160 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ]]()\n",
      "  %161 : Float(1, 512, 4, *, device=cpu) = onnx::Pad[mode=\"constant\"](%159, %160)\n",
      "  %162 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, kernel_shape=[2, 1], pads=[0, 0, 0, 0], strides=[2, 1]](%161) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/modules/pooling.py:616:0\n",
      "  %225 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%162, %226, %227)\n",
      "  %165 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%225) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %228 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%165, %229, %230)\n",
      "  %168 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%228) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %231 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%168, %232, %233)\n",
      "  %171 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%231) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %234 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%171, %235, %236)\n",
      "  %174 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu(%234) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %175 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ]]()\n",
      "  %176 : Float(1, 512, 2, *, device=cpu) = onnx::Pad[mode=\"constant\"](%174, %175)\n",
      "  %177 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%176) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/functional.py:1076:0\n",
      "  %178 : Float(1, 256, 2, *, strides=[60416, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%177, %model.last_conv_1x1.weight, %model.last_conv_1x1.bias) # /home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/nn/modules/conv.py:396:0\n",
      "  %179 : Float(1, 256, *, 2, strides=[60416, 236, 1, 118], requires_grad=0, device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2]](%178) # /home/manhbui/manhbq_workspace/ConvertVietOcr2Onnx/model/backbone/vgg.py:40:0\n",
      "  %180 : Long(4, strides=[1], device=cpu) = onnx::Shape(%179)\n",
      "  %181 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %182 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %183 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={2}]()\n",
      "  %184 : Long(2, strides=[1], device=cpu) = onnx::Slice(%180, %182, %183, %181)\n",
      "  %185 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %186 : Long(3, strides=[1], device=cpu) = onnx::Concat[axis=0](%184, %185)\n",
      "  %187 : Float(1, 256, 236, strides=[60416, 236, 1], requires_grad=0, device=cpu) = onnx::Reshape(%179, %186) # /home/manhbui/manhbq_workspace/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %output : Float(236, 1, 256, strides=[1, 60416, 236], requires_grad=0, device=cpu) = onnx::Transpose[perm=[2, 0, 1]](%187) # /home/manhbui/manhbq_workspace/ConvertVietOcr2Onnx/model/backbone/vgg.py:42:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand(1, 3, 32, 475)\n",
    "src = convert_cnn_part(img, './weight/cnn.onnx', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoder_part(model, src, save_path): \n",
    "    encoder_outputs, hidden = model.transformer.encoder(src) \n",
    "    torch.onnx.export(model.transformer.encoder, src, save_path, export_params=True, opset_version=11, do_constant_folding=True, input_names=['src'], output_names=['encoder_outputs', 'hidden'], dynamic_axes={'src':{0: \"channel_input\"}, 'encoder_outputs': {0: 'channel_output'}}) \n",
    "    return hidden, encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manhbui/anaconda3/envs/manhbq/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:1945: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
     ]
    }
   ],
   "source": [
    "hidden, encoder_outputs = convert_encoder_part(model, src, './weight/encoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decoder_part(model, tgt, hidden, encoder_outputs, save_path):\n",
    "    tgt = tgt[-1]\n",
    "    \n",
    "    torch.onnx.export(model.transformer.decoder,\n",
    "        (tgt, hidden, encoder_outputs),\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['tgt', 'hidden', 'encoder_outputs'],\n",
    "        output_names=['output', 'hidden_out', 'last'],\n",
    "        dynamic_axes={'encoder_outputs':{0: \"channel_input\"},\n",
    "                    'last': {0: 'channel_output'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = img.device\n",
    "tgt = torch.LongTensor([[1] * len(img)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manhbui/manhbq_workspace/ConvertVietOcr2Onnx/model/seqmodel/seq2seq.py:93: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (output == hidden).all()\n"
     ]
    }
   ],
   "source": [
    "convert_decoder_part(model, tgt, hidden, encoder_outputs, './weight/decoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = onnx.load('./weight/cnn.onnx')\n",
    "decoder = onnx.load('./weight/encoder.onnx')\n",
    "encoder = onnx.load('./weight/decoder.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm model has valid schema\n",
    "onnx.checker.check_model(cnn)\n",
    "onnx.checker.check_model(decoder)\n",
    "onnx.checker.check_model(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'graph torch-jit-export (\\n  %tgt[INT64, 1]\\n  %hidden[FLOAT, 1x256]\\n  %encoder_outputs[FLOAT, channel_inputx1x512]\\n) initializers (\\n  %attention.attn.bias[FLOAT, 256]\\n  %embedding.weight[FLOAT, 233x256]\\n  %fc_out.weight[FLOAT, 233x1024]\\n  %fc_out.bias[FLOAT, 233]\\n  %116[INT64, 1]\\n  %117[INT64, 1]\\n  %118[INT64, 1]\\n  %119[INT64, 1]\\n  %120[FLOAT, 768x256]\\n  %121[FLOAT, 256x1]\\n  %139[FLOAT, 1x768x768]\\n  %140[FLOAT, 1x768x256]\\n  %141[FLOAT, 1x1536]\\n) {\\n  %13 = Unsqueeze[axes = [0]](%tgt)\\n  %14 = Gather(%embedding.weight, %13)\\n  %15 = Shape(%encoder_outputs)\\n  %16 = Constant[value = <Scalar Tensor []>]()\\n  %17 = Gather[axis = 0](%15, %16)\\n  %18 = Unsqueeze[axes = [1]](%hidden)\\n  %22 = Unsqueeze[axes = [0]](%17)\\n  %24 = Concat[axis = 0](%116, %22, %117)\\n  %26 = Unsqueeze[axes = [0]](%17)\\n  %28 = Concat[axis = 0](%118, %26, %119)\\n  %29 = Shape(%24)\\n  %30 = ConstantOfShape[value = <Tensor>](%29)\\n  %31 = Expand(%18, %30)\\n  %32 = Tile(%31, %28)\\n  %33 = Transpose[perm = [1, 0, 2]](%encoder_outputs)\\n  %34 = Concat[axis = 2](%32, %33)\\n  %36 = MatMul(%34, %120)\\n  %37 = Add(%36, %attention.attn.bias)\\n  %38 = Tanh(%37)\\n  %40 = MatMul(%38, %121)\\n  %41 = Squeeze[axes = [2]](%40)\\n  %42 = Softmax[axis = 1](%41)\\n  %43 = Unsqueeze[axes = [1]](%42)\\n  %44 = Transpose[perm = [1, 0, 2]](%encoder_outputs)\\n  %45 = MatMul(%43, %44)\\n  %46 = Transpose[perm = [1, 0, 2]](%45)\\n  %47 = Concat[axis = 2](%14, %46)\\n  %48 = Unsqueeze[axes = [0]](%hidden)\\n  %106, %107 = GRU[hidden_size = 256, linear_before_reset = 1](%47, %139, %140, %141, %, %48)\\n  %108 = Squeeze[axes = [1]](%106)\\n  %109 = Squeeze[axes = [0]](%14)\\n  %110 = Squeeze[axes = [0]](%108)\\n  %111 = Squeeze[axes = [0]](%46)\\n  %112 = Concat[axis = 1](%110, %111, %109)\\n  %output = Gemm[alpha = 1, beta = 1, transB = 1](%112, %fc_out.weight, %fc_out.bias)\\n  %hidden_out = Squeeze[axes = [0]](%107)\\n  %last = Squeeze[axes = [1]](%43)\\n  return %output, %hidden_out, %last\\n}'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(encoder.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./sample/35944.png')\n",
    "img = process_input(img, config['dataset']['image_height'], \n",
    "                config['dataset']['image_min_width'], config['dataset']['image_max_width'])  \n",
    "img = img.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Mâm non: 141 thí sinh'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = translate(img, model)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with ONNX Runtime's Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference session\n",
    "cnn_session = onnxruntime.InferenceSession(\"./weight/cnn.onnx\")\n",
    "encoder_session = onnxruntime.InferenceSession(\"./weight/encoder.onnx\")\n",
    "decoder_session = onnxruntime.InferenceSession(\"./weight/decoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_onnx(img, session, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    \"\"\"data: BxCxHxW\"\"\"\n",
    "    cnn_session, encoder_session, decoder_session = session\n",
    "    \n",
    "    # create cnn input\n",
    "    cnn_input = {cnn_session.get_inputs()[0].name: img}\n",
    "    src = cnn_session.run(None, cnn_input)\n",
    "    \n",
    "    # create encoder input\n",
    "    encoder_input = {encoder_session.get_inputs()[0].name: src[0]}\n",
    "    encoder_outputs, hidden = encoder_session.run(None, encoder_input)\n",
    "    translated_sentence = [[sos_token] * len(img)]\n",
    "    max_length = 0\n",
    "\n",
    "    while max_length <= max_seq_length and not all(\n",
    "        np.any(np.asarray(translated_sentence).T == eos_token, axis=1)\n",
    "    ):\n",
    "        tgt_inp = translated_sentence\n",
    "        decoder_input = {decoder_session.get_inputs()[0].name: tgt_inp[-1], decoder_session.get_inputs()[1].name: hidden, decoder_session.get_inputs()[2].name: encoder_outputs}\n",
    "\n",
    "        output, hidden, _ = decoder_session.run(None, decoder_input)\n",
    "        output = np.expand_dims(output, axis=1)\n",
    "        output = torch.Tensor(output)\n",
    "\n",
    "        values, indices = torch.topk(output, 1)\n",
    "        indices = indices[:, -1, 0]\n",
    "        indices = indices.tolist()\n",
    "\n",
    "        translated_sentence.append(indices)\n",
    "        max_length += 1\n",
    "\n",
    "        del output\n",
    "\n",
    "    translated_sentence = np.asarray(translated_sentence).T\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Mâm non: 141 thí sinh'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = (cnn_session, encoder_session, decoder_session)\n",
    "s = translate_onnx(np.array(img), session)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('manhbq': conda)",
   "name": "python3710jvsc74a57bd08073656a449d74c1402c8646685842603f61a524b0c74948679a8c6893091938"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}